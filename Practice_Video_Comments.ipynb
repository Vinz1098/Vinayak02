{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO71KlCFrcQMqS8VyFu14Ki",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vinz1098/Vinayak02/blob/main/Practice_Video_Comments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgzlHCA-G_OR",
        "outputId": "fc9edc1d-7187-4329-b5da-27b7147d8659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (2.84.0)\n",
            "Collecting google-api-python-client\n",
            "  Downloading google_api_python_client-2.106.0-py2.py3-none-any.whl (12.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Collecting google-auth-oauthlib\n",
            "  Downloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth<3.0.0.dev0,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.17.3)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib) (1.3.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.61.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.31.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client) (3.1.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2023.7.22)\n",
            "Installing collected packages: google-auth-oauthlib, google-api-python-client\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.0.0\n",
            "    Uninstalling google-auth-oauthlib-1.0.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 2.84.0\n",
            "    Uninstalling google-api-python-client-2.84.0:\n",
            "      Successfully uninstalled google-api-python-client-2.84.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorboard 2.14.1 requires google-auth-oauthlib<1.1,>=0.5, but you have google-auth-oauthlib 1.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-api-python-client-2.106.0 google-auth-oauthlib-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MdMA6sxoyLCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the YouTube Data API as described in previous responses.\n",
        "\n",
        "# Import the necessary modules and authenticate with your API key.\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# Replace 'YOUR_API_KEY' with your actual API key.\n",
        "api_key = 'AIzaSyCfTwWKjCXEs3lEdvx2GwKHdngxw4IU4-0'\n",
        "\n",
        "youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "\n",
        "# Replace 'CHANNEL_ID' with the channel ID you want to retrieve videos for.\n",
        "channel_id = 'UCpgVacgMNP3OG8PcWnHbLdg'\n",
        "\n",
        "# Retrieve the video data for the channel.\n",
        "videos = []\n",
        "\n",
        "next_page_token = None\n",
        "while True:\n",
        "    response = youtube.search().list(\n",
        "        part='id',\n",
        "        channelId=channel_id,\n",
        "        maxResults=50,  # You can adjust this based on your needs.\n",
        "        order='date',\n",
        "        pageToken=next_page_token,\n",
        "        type='video'\n",
        "    ).execute()\n",
        "\n",
        "    video_ids = [item['id']['videoId'] for item in response['items']]\n",
        "\n",
        "    video_response = youtube.videos().list(\n",
        "        part='snippet,statistics',\n",
        "        id=','.join(video_ids)\n",
        "    ).execute()\n",
        "\n",
        "    videos.extend(video_response['items'])\n",
        "\n",
        "    next_page_token = response.get('nextPageToken')\n",
        "\n",
        "    if not next_page_token:\n",
        "        break\n",
        "\n",
        "# Retrieve comments for each video and save them to a CSV file.\n",
        "import csv\n",
        "\n",
        "csv_file_name = 'video_comments.csv'\n",
        "\n",
        "with open(csv_file_name, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Video Title', 'Video Description', 'Video Views', 'Video Likes', 'Video Comments', 'Comment'])\n",
        "\n",
        "    for video in videos:\n",
        "        video_id = video['id']\n",
        "        comments = []\n",
        "\n",
        "        response = youtube.commentThreads().list(\n",
        "            part='snippet',\n",
        "            videoId=video_id,\n",
        "            maxResults=100,  # You can adjust this based on your needs.\n",
        "        ).execute()\n",
        "\n",
        "        for item in response['items']:\n",
        "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "            comments.append(comment)\n",
        "\n",
        "        video_info = [\n",
        "            video['snippet']['title'],\n",
        "            video['snippet']['description'],\n",
        "            video['statistics']['viewCount'],\n",
        "            video['statistics']['likeCount'],\n",
        "            video['statistics']['commentCount']\n",
        "        ]\n",
        "\n",
        "        for comment in comments:\n",
        "            writer.writerow(video_info + [comment])\n",
        "\n",
        "# Verify and download the CSV file.\n",
        "import shutil\n",
        "\n",
        "shutil.move(csv_file_name, \"/content/\" + csv_file_name)\n",
        "\n",
        "import os\n",
        "\n",
        "os.listdir(\"/content\")\n"
      ],
      "metadata": {
        "id": "iustvuW5yLGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the YouTube Data API as described in previous responses.\n",
        "\n",
        "# Import the necessary modules and authenticate with your API key.\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# Replace 'YOUR_API_KEY' with your actual API key.\n",
        "api_key = 'AIzaSyCfTwWKjCXEs3lEdvx2GwKHdngxw4IU4-0'\n",
        "\n",
        "youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "\n",
        "# Replace 'CHANNEL_ID' with the channel ID you want to retrieve videos for.\n",
        "channel_id = 'UCpgVacgMNP3OG8PcWnHbLdg'\n",
        "\n",
        "# Retrieve the video data for the channel.\n",
        "videos = []\n",
        "\n",
        "next_page_token = None\n",
        "while True:\n",
        "    response = youtube.search().list(\n",
        "        part='id',\n",
        "        channelId=channel_id,\n",
        "        maxResults=50,  # You can adjust this based on your needs.\n",
        "        order='date',\n",
        "        pageToken=next_page_token,\n",
        "        type='video'\n",
        "    ).execute()\n",
        "\n",
        "    video_ids = [item['id']['videoId'] for item in response['items']]\n",
        "\n",
        "    video_response = youtube.videos().list(\n",
        "        part='snippet,statistics',\n",
        "        id=','.join(video_ids)\n",
        "    ).execute()\n",
        "\n",
        "    videos.extend(video_response['items'])\n",
        "\n",
        "    next_page_token = response.get('nextPageToken')\n",
        "\n",
        "    if not next_page_token:\n",
        "        break\n",
        "\n",
        "# Retrieve comments for each video and save them to a CSV file.\n",
        "import csv\n",
        "\n",
        "csv_file_name = 'video_comments.csv'\n",
        "\n",
        "with open(csv_file_name, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Video Title', 'Video Description', 'Video Views', 'Video Likes', 'Video Comments', 'Comment'])\n",
        "\n",
        "    for video in videos:\n",
        "        video_id = video['id']\n",
        "        comments = []\n",
        "\n",
        "        response = youtube.commentThreads().list(\n",
        "            part='snippet',\n",
        "            videoId=video_id,\n",
        "            maxResults=100,  # You can adjust this based on your needs.\n",
        "        ).execute()\n",
        "\n",
        "        for item in response['items']:\n",
        "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "            comments.append(comment)\n",
        "\n",
        "        video_info = [\n",
        "            video['snippet']['title'],\n",
        "            video['snippet']['description'],\n",
        "            video['statistics']['viewCount'],\n",
        "            video['statistics']['likeCount'],\n",
        "            video['statistics']['commentCount']\n",
        "        ]\n",
        "\n",
        "        for comment in comments:\n",
        "            writer.writerow(video_info + [comment])\n",
        "\n",
        "# Verify and download the CSV file.\n",
        "import shutil\n",
        "\n",
        "shutil.move(csv_file_name, \"/content/\" + csv_file_name)\n",
        "\n",
        "import os\n",
        "\n",
        "os.listdir(\"/content\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p6Q-7BtP45R",
        "outputId": "1d52a2cc-207b-447d-efeb-b681bbb02688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', '.ipynb_checkpoints', 'video_comments.csv', 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the YouTube Data API as described in previous responses.\n",
        "\n",
        "# Import the necessary modules and authenticate with your API key.\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# Replace 'YOUR_API_KEY' with your actual API key.\n",
        "api_key = 'AIzaSyCfTwWKjCXEs3lEdvx2GwKHdngxw4IU4-0'\n",
        "\n",
        "youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "\n",
        "# Replace 'CHANNEL_ID' with the channel ID you want to retrieve videos for.\n",
        "channel_id = 'UCpgVacgMNP3OG8PcWnHbLdg'\n",
        "\n",
        "# Retrieve the video data for the channel.\n",
        "videos = []\n",
        "\n",
        "next_page_token = None\n",
        "while True:\n",
        "    response = youtube.search().list(\n",
        "        part='id',\n",
        "        channelId=channel_id,\n",
        "        maxResults=50,  # You can adjust this based on your needs.\n",
        "        order='date',\n",
        "        pageToken=next_page_token,\n",
        "        type='video'\n",
        "    ).execute()\n",
        "\n",
        "    video_ids = [item['id']['videoId'] for item in response['items']]\n",
        "\n",
        "    video_response = youtube.videos().list(\n",
        "        part='snippet,statistics',\n",
        "        id=','.join(video_ids)\n",
        "    ).execute()\n",
        "\n",
        "    videos.extend(video_response['items'])\n",
        "\n",
        "    next_page_token = response.get('nextPageToken')\n",
        "\n",
        "    if not next_page_token:\n",
        "        break\n",
        "\n",
        "# Create a CSV file to save the data.\n",
        "csv_file_name = 'video_comments.csv'\n",
        "\n",
        "# Open the CSV file for writing.\n",
        "with open(csv_file_name, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Create headers for the CSV file.\n",
        "    headers = ['Video Title', 'Video Description', 'Video Views', 'Video Likes', 'Video Comments']\n",
        "    # Create additional headers for comments.\n",
        "    for i in range(1, 101):  # Assuming a maximum of 100 comments per video.\n",
        "        headers.append(f'Comment {i}')\n",
        "    writer.writerow(headers)\n",
        "\n",
        "    for video in videos:\n",
        "        video_id = video['id']\n",
        "        comments = []\n",
        "\n",
        "        response = youtube.commentThreads().list(\n",
        "            part='snippet',\n",
        "            videoId=video_id,\n",
        "            maxResults=100,  # You can adjust this based on your needs.\n",
        "        ).execute()\n",
        "\n",
        "        for item in response['items']:\n",
        "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "            comments.append(comment)\n",
        "\n",
        "        # Prepare the row to write to the CSV file.\n",
        "        row = [\n",
        "            video['snippet']['title'],\n",
        "            video['snippet']['description'],\n",
        "            video['statistics']['viewCount'],\n",
        "            video['statistics']['likeCount'],\n",
        "            video['statistics']['commentCount']\n",
        "        ]\n",
        "\n",
        "        # Extend the row with comments.\n",
        "        row.extend(comments)\n",
        "\n",
        "        # Fill any remaining columns with empty strings to maintain consistency.\n",
        "        while len(row) < len(headers):\n",
        "            row.append('')\n",
        "\n",
        "        writer.writerow(row)\n",
        "\n",
        "# Verify and download the CSV file.\n",
        "import shutil\n",
        "shutil.move(csv_file_name, \"/content/\" + csv_file_name)\n",
        "\n",
        "import os\n",
        "os.listdir(\"/content\")"
      ],
      "metadata": {
        "id": "6vL4666HQFqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the YouTube Data API as described in previous responses.\n",
        "\n",
        "# Import the necessary modules and authenticate with your API key.\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# Replace 'YOUR_API_KEY' with your actual API key.\n",
        "api_key = 'AIzaSyCfTwWKjCXEs3lEdvx2GwKHdngxw4IU4-0'\n",
        "\n",
        "youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "\n",
        "# Replace 'CHANNEL_ID' with the channel ID you want to retrieve videos for.\n",
        "channel_id = 'UCJfmMqaqfT7rcZJFDemihug'\n",
        "\n",
        "# Retrieve the video data for the channel.\n",
        "videos = []\n",
        "\n",
        "next_page_token = None\n",
        "while True:\n",
        "    response = youtube.search().list(\n",
        "        part='id',\n",
        "        channelId=channel_id,\n",
        "        maxResults=50,  # You can adjust this based on your needs.\n",
        "        order='date',\n",
        "        pageToken=next_page_token,\n",
        "        type='video'\n",
        "    ).execute()\n",
        "\n",
        "    video_ids = [item['id']['videoId'] for item in response['items']]\n",
        "\n",
        "    video_response = youtube.videos().list(\n",
        "        part='snippet,statistics',\n",
        "        id=','.join(video_ids)\n",
        "    ).execute()\n",
        "\n",
        "    videos.extend(video_response['items'])\n",
        "\n",
        "    next_page_token = response.get('nextPageToken')\n",
        "\n",
        "    if not next_page_token:\n",
        "        break\n",
        "\n",
        "# Retrieve comments for each video and save them to a CSV file.\n",
        "import csv\n",
        "\n",
        "csv_file_name = 'video_comments.csv'\n",
        "\n",
        "with open(csv_file_name, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Video Title', 'Video Description', 'Video Views', 'Video Likes', 'Video Comments', 'Comment'])\n",
        "\n",
        "    for video in videos:\n",
        "        video_id = video['id']\n",
        "        comments = []\n",
        "\n",
        "        response = youtube.commentThreads().list(\n",
        "            part='snippet',\n",
        "            videoId=video_id,\n",
        "            maxResults=100,  # You can adjust this based on your needs.\n",
        "        ).execute()\n",
        "\n",
        "        for item in response['items']:\n",
        "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "            comments.append(comment)\n",
        "\n",
        "        video_info = [\n",
        "            video['snippet']['title'],\n",
        "            video['snippet']['description'],\n",
        "            video['statistics']['viewCount'],\n",
        "            video['statistics']['likeCount'],\n",
        "            video['statistics']['commentCount']\n",
        "        ]\n",
        "\n",
        "        for comment in comments:\n",
        "            writer.writerow(video_info + [comment])\n",
        "\n",
        "# Verify and download the CSV file.\n",
        "import shutil\n",
        "\n",
        "shutil.move(csv_file_name, \"/content/\" + csv_file_name)\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "os.listdir(\"/content\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmPcUbXzTt05",
        "outputId": "de757ea6-2cac-4487-afc8-9e6681e5ab34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', '.ipynb_checkpoints', 'video_comments.csv', 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the YouTube Data API as described in previous responses.\n",
        "\n",
        "# Import the necessary modules and authenticate with your API key.\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# Replace 'YOUR_API_KEY' with your actual API key.\n",
        "api_key = 'AIzaSyCfTwWKjCXEs3lEdvx2GwKHdngxw4IU4-0'\n",
        "\n",
        "youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "\n",
        "# Replace 'CHANNEL_ID' with the channel ID you want to retrieve videos for.\n",
        "channel_id = 'UCJfmMqaqfT7rcZJFDemihug'\n",
        "\n",
        "# Retrieve the video data for the channel.\n",
        "videos = []\n",
        "\n",
        "next_page_token = None\n",
        "while True:\n",
        "    response = youtube.search().list(\n",
        "        part='id',\n",
        "        channelId=channel_id,\n",
        "        maxResults=50,  # You can adjust this based on your needs.\n",
        "        order='date',\n",
        "        pageToken=next_page_token,\n",
        "        type='video'\n",
        "    ).execute()\n",
        "\n",
        "    video_ids = [item['id']['videoId'] for item in response['items']]\n",
        "\n",
        "    video_response = youtube.videos().list(\n",
        "        part='snippet,statistics',\n",
        "        id=','.join(video_ids)\n",
        "    ).execute()\n",
        "\n",
        "    videos.extend(video_response['items'])\n",
        "\n",
        "    next_page_token = response.get('nextPageToken')\n",
        "\n",
        "    if not next_page_token:\n",
        "        break\n",
        "\n",
        "# Create a CSV file to save the data.\n",
        "csv_file_name = 'video_comments.csv'\n",
        "\n",
        "# Open the CSV file for writing.\n",
        "with open(csv_file_name, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Create headers for the CSV file.\n",
        "    headers = ['Video Title', 'Video Description', 'Video Views', 'Video Likes', 'Video Comments']\n",
        "    # Create additional headers for comments.\n",
        "    for i in range(1, 101):  # Assuming a maximum of 100 comments per video.\n",
        "        headers.append(f'Comment {i}')\n",
        "    writer.writerow(headers)\n",
        "\n",
        "    for video in videos:\n",
        "        video_id = video['id']\n",
        "        comments = []\n",
        "\n",
        "        response = youtube.commentThreads().list(\n",
        "            part='snippet',\n",
        "            videoId=video_id,\n",
        "            maxResults=100,  # You can adjust this based on your needs.\n",
        "        ).execute()\n",
        "\n",
        "        for item in response['items']:\n",
        "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "            comments.append(comment)\n",
        "\n",
        "        # Prepare the row to write to the CSV file.\n",
        "        row = [\n",
        "            video['snippet']['title'],\n",
        "            video['snippet']['description'],\n",
        "            video['statistics']['viewCount'],\n",
        "            video['statistics']['likeCount'],\n",
        "            video['statistics']['commentCount']\n",
        "        ]\n",
        "\n",
        "        # Extend the row with comments.\n",
        "        row.extend(comments)\n",
        "\n",
        "        # Fill any remaining columns with empty strings to maintain consistency.\n",
        "        while len(row) < len(headers):\n",
        "            row.append('')\n",
        "\n",
        "        writer.writerow(row)\n",
        "\n",
        "# Verify and download the CSV file.\n",
        "import shutil\n",
        "shutil.move(csv_file_name, \"/content/\" + csv_file_name)\n",
        "\n",
        "import os\n",
        "os.listdir(\"/content\")"
      ],
      "metadata": {
        "id": "FkttdUpZVJO6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "a1eedaff-0069-426c-805f-ee1e8f49b1d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-cc0067bd21cc>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Open the CSV file for writing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Create headers for the CSV file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'csv' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the YouTube Data API as described in previous responses.\n",
        "\n",
        "# Import the necessary modules and authenticate with your API key.\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# Replace 'YOUR_API_KEY' with your actual API key.\n",
        "api_key = 'AIzaSyCfTwWKjCXEs3lEdvx2GwKHdngxw4IU4-0'\n",
        "\n",
        "youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "\n",
        "# Replace 'CHANNEL_ID' with the channel ID you want to retrieve videos for.\n",
        "channel_id = 'UCpgVacgMNP3OG8PcWnHbLdg'\n",
        "\n",
        "# Retrieve the video data for the channel.\n",
        "videos = []\n",
        "\n",
        "next_page_token = None\n",
        "while True:\n",
        "    response = youtube.search().list(\n",
        "        part='id',\n",
        "        channelId=channel_id,\n",
        "        maxResults=50,  # You can adjust this based on your needs.\n",
        "        order='date',\n",
        "        pageToken=next_page_token,\n",
        "        type='video'\n",
        "    ).execute()\n",
        "\n",
        "    video_ids = [item['id']['videoId'] for item in response['items']]\n",
        "\n",
        "    video_response = youtube.videos().list(\n",
        "        part='snippet,statistics',\n",
        "        id=','.join(video_ids)\n",
        "    ).execute()\n",
        "\n",
        "    videos.extend(video_response['items'])\n",
        "\n",
        "    next_page_token = response.get('nextPageToken')\n",
        "\n",
        "    if not next_page_token:\n",
        "        break\n",
        "\n",
        "# Create a CSV file to save the data.\n",
        "csv_file_name = 'video_comments.csv'\n",
        "\n",
        "# Open the CSV file for writing.\n",
        "with open(csv_file_name, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Create headers for the CSV file.\n",
        "    headers = ['Video Title', 'Video Description', 'Video Views', 'Video Likes', 'Video Comments']\n",
        "    # Create additional headers for comments.\n",
        "    for i in range(1, 101):  # Assuming a maximum of 100 comments per video.\n",
        "        headers.append(f'Comment {i}')\n",
        "    writer.writerow(headers)\n",
        "\n",
        "    for video in videos:\n",
        "        video_id = video['id']\n",
        "        comments = []\n",
        "\n",
        "        response = youtube.commentThreads().list(\n",
        "            part='snippet',\n",
        "            videoId=video_id,\n",
        "            maxResults=100,  # You can adjust this based on your needs.\n",
        "        ).execute()\n",
        "\n",
        "        for item in response['items']:\n",
        "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "            comments.append(comment)\n",
        "\n",
        "        # Prepare the row to write to the CSV file.\n",
        "        row = [\n",
        "            video['snippet']['title'],\n",
        "            video['snippet']['description'],\n",
        "            video['statistics']['viewCount'],\n",
        "            video['statistics']['likeCount'],\n",
        "            video['statistics']['commentCount']\n",
        "        ]\n",
        "\n",
        "        # Extend the row with comments.\n",
        "        row.extend(comments)\n",
        "\n",
        "        # Fill any remaining columns with empty strings to maintain consistency.\n",
        "        while len(row) < len(headers):\n",
        "            row.append('')\n",
        "\n",
        "        writer.writerow(row)\n",
        "\n",
        "# Verify and download the CSV file.\n",
        "import shutil\n",
        "shutil.move(csv_file_name, \"/content/\" + csv_file_name)\n",
        "\n",
        "import os\n",
        "os.listdir(\"/content\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "uOIdwjRDdD1_",
        "outputId": "dbe974b7-55bf-48b7-c4c8-1dd062d07223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-281d6b948083>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Open the CSV file for writing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Create headers for the CSV file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'csv' is not defined"
          ]
        }
      ]
    }
  ]
}